{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3187731,"sourceType":"datasetVersion","datasetId":1935792},{"sourceId":8384660,"sourceType":"datasetVersion","datasetId":4986694}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Importing libraries**","metadata":{"id":"ZsKYkocGm0La"}},{"cell_type":"code","source":"import os\nimport shutil\nimport cv2\nimport math\nimport random\nimport numpy as np\nimport datetime as dt\nimport tensorflow \nimport keras\nfrom collections import deque\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\n%matplotlib inline\n \nfrom sklearn.model_selection import train_test_split\n \nfrom keras.layers import *\nfrom keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import plot_model","metadata":{"id":"KN35l9ZJ3aTh","execution":{"iopub.status.busy":"2024-05-11T13:46:48.314951Z","iopub.execute_input":"2024-05-11T13:46:48.315858Z","iopub.status.idle":"2024-05-11T13:46:48.328870Z","shell.execute_reply.started":"2024-05-11T13:46:48.315822Z","shell.execute_reply":"2024-05-11T13:46:48.327999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualize the Data**","metadata":{"id":"2JxJyU0e3aTn"}},{"cell_type":"code","source":"from IPython.display import HTML\nfrom base64 import b64encode\n\n# To Show a Video in Notebook\ndef Play_Video(filepath):\n    html = ''\n    video = open(filepath,'rb').read()\n    src = 'data:video/mp4;base64,' + b64encode(video).decode()\n    html += '<video width=640 muted controls autoplay loop><source src=\"%s\" type=\"video/mp4\"></video>' % src \n    return HTML(html)","metadata":{"id":"Sn7l5xJlmtrf","execution":{"iopub.status.busy":"2024-05-11T13:41:26.127602Z","iopub.execute_input":"2024-05-11T13:41:26.128575Z","iopub.status.idle":"2024-05-11T13:41:26.134424Z","shell.execute_reply.started":"2024-05-11T13:41:26.128541Z","shell.execute_reply":"2024-05-11T13:41:26.133492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classes Directories\nNonViolnceVideos_Dir = r\"/kaggle/input/violencedetectiondataset/violence-detection-dataset/non-violent\"\nViolnceVideos_Dir = r\"/kaggle/input/violencedetectiondataset/violence-detection-dataset/violent\"\n\n# Retrieve the list of all the video files present in the Class Directory.\nNonViolence_files_names_list = os.listdir(NonViolnceVideos_Dir)\nViolence_files_names_list = os.listdir(ViolnceVideos_Dir)\n\n# Randomly select a video file from the Classes Directory.\nRandom_NonViolence_Video = random.choice(NonViolence_files_names_list)\nRandom_Violence_Video = random.choice(Violence_files_names_list)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"WzJy3lsU3aTp","execution":{"iopub.status.busy":"2024-05-11T13:41:58.808613Z","iopub.execute_input":"2024-05-11T13:41:58.809460Z","iopub.status.idle":"2024-05-11T13:41:58.878193Z","shell.execute_reply.started":"2024-05-11T13:41:58.809430Z","shell.execute_reply":"2024-05-11T13:41:58.877293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Play Random Non Violence Video**","metadata":{"id":"pq5KrFGd-HJ6"}},{"cell_type":"code","source":"Play_Video(f\"{NonViolnceVideos_Dir}/{Random_NonViolence_Video}\")","metadata":{"id":"P2wg6bCP9C3S","outputId":"cec080f1-ffc1-4343-8933-49b8ca755bbb","execution":{"iopub.status.busy":"2024-05-11T13:42:15.421814Z","iopub.execute_input":"2024-05-11T13:42:15.422540Z","iopub.status.idle":"2024-05-11T13:42:15.526234Z","shell.execute_reply.started":"2024-05-11T13:42:15.422509Z","shell.execute_reply":"2024-05-11T13:42:15.524604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Play Random Violence Video**","metadata":{"id":"eDllFj2y-LXj"}},{"cell_type":"code","source":"Play_Video(f\"{ViolnceVideos_Dir}/{Random_Violence_Video}\")","metadata":{"id":"AdbW5DIYnk2D","outputId":"8b2d7e38-ba3d-4e97-9458-89e16ff24c59","execution":{"iopub.status.busy":"2024-05-11T13:42:25.407369Z","iopub.execute_input":"2024-05-11T13:42:25.407782Z","iopub.status.idle":"2024-05-11T13:42:25.530737Z","shell.execute_reply.started":"2024-05-11T13:42:25.407748Z","shell.execute_reply":"2024-05-11T13:42:25.529710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Extracting Frames**","metadata":{"id":"ySOhHqy83aTq"}},{"cell_type":"code","source":"# Specify the height and width to which each video frame will be resized in our dataset.\nIMAGE_HEIGHT , IMAGE_WIDTH = 64, 64\n \n# Specify the number of frames of a video that will be fed to the model as one sequence.\nSEQUENCE_LENGTH = 16\n \n\nDATASET_DIR = r'/kaggle/input/violencedetectiondataset/violence-detection-dataset'\n \nCLASSES_LIST = [\"non-violent\",\"violent\"]","metadata":{"id":"1arHoOHI3aTr","execution":{"iopub.status.busy":"2024-05-11T13:42:58.079582Z","iopub.execute_input":"2024-05-11T13:42:58.080543Z","iopub.status.idle":"2024-05-11T13:42:58.085124Z","shell.execute_reply.started":"2024-05-11T13:42:58.080504Z","shell.execute_reply":"2024-05-11T13:42:58.084209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def frames_extraction(video_path):\n \n    frames_list = []\n    \n    # Read the Video File\n    video_reader = cv2.VideoCapture(video_path)\n \n    # Get the total number of frames in the video.\n    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n \n    # Calculate the the interval after which frames will be added to the list.\n    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n \n    # Iterate through the Video Frames.\n    for frame_counter in range(SEQUENCE_LENGTH):\n \n        # Set the current frame position of the video.\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n \n        # Reading the frame from the video. \n        success, frame = video_reader.read() \n \n        if not success:\n            break\n \n        # Resize the Frame to fixed height and width.\n        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        \n        # Normalize the resized frame\n        normalized_frame = resized_frame / 255\n        \n        # Append the normalized frame into the frames list\n        frames_list.append(normalized_frame)\n    \n \n    video_reader.release()\n \n    return frames_list","metadata":{"id":"-yB8ePeC3aTs","execution":{"iopub.status.busy":"2024-05-11T13:43:02.551015Z","iopub.execute_input":"2024-05-11T13:43:02.551391Z","iopub.status.idle":"2024-05-11T13:43:02.559547Z","shell.execute_reply.started":"2024-05-11T13:43:02.551349Z","shell.execute_reply":"2024-05-11T13:43:02.558469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Creating Our DataSet**","metadata":{"id":"uqAqiZxD3aTt"}},{"cell_type":"code","source":"def create_dataset():\n \n    features = []\n    labels = []\n    video_files_paths = []\n    \n    # Iterating through all the classes.\n    for class_index, class_name in enumerate(CLASSES_LIST):\n        \n        print(f'Extracting Data of Class: {class_name}')\n        \n        # Get the list of video files present in the specific class name directory.\n        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n        \n        # Iterate through all the files present in the files list.\n        for file_name in tqdm(files_list):\n            \n            # Get the complete video path.\n            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n \n            # Extract the frames of the video file.\n            frames = frames_extraction(video_file_path)\n \n            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified.\n            # So ignore the videos having frames less than the SEQUENCE_LENGTH.\n            if len(frames) == SEQUENCE_LENGTH:\n \n                # Append the data to their respective lists.\n                features.append(frames)\n                labels.append(class_index)\n                video_files_paths.append(video_file_path)\n \n    features = np.asarray(features)\n    labels = np.array(labels)  \n\n    return features, labels, video_files_paths","metadata":{"id":"vj_AQqju3aTu","execution":{"iopub.status.busy":"2024-05-11T13:50:18.651655Z","iopub.execute_input":"2024-05-11T13:50:18.652034Z","iopub.status.idle":"2024-05-11T13:50:18.659907Z","shell.execute_reply.started":"2024-05-11T13:50:18.652008Z","shell.execute_reply":"2024-05-11T13:50:18.658832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checks if directory exists \nimport os\n\ndirectory_path = r'/kaggle/input/violencedetectiondataset/violence-detection-dataset'\n\nif os.path.exists(directory_path):\n    print (\"found\")\nelse:\n    print(f\"The directory {directory_path} does not exist.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:50:19.769393Z","iopub.execute_input":"2024-05-11T13:50:19.770376Z","iopub.status.idle":"2024-05-11T13:50:19.778056Z","shell.execute_reply.started":"2024-05-11T13:50:19.770338Z","shell.execute_reply":"2024-05-11T13:50:19.777153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the Extracted dataset.\nfeatures, labels, video_files_paths = create_dataset()","metadata":{"id":"rgpokUY83aTv","execution":{"iopub.status.busy":"2024-05-11T13:50:20.368987Z","iopub.execute_input":"2024-05-11T13:50:20.369816Z","iopub.status.idle":"2024-05-11T14:22:29.821518Z","shell.execute_reply.started":"2024-05-11T13:50:20.369779Z","shell.execute_reply":"2024-05-11T14:22:29.820678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the extracted data\nnp.save(\"features.npy\",features)\nnp.save(\"labels.npy\",labels)\nnp.save(\"video_files_paths.npy\",video_files_paths)","metadata":{"id":"Hu8NKv4H3aTv","execution":{"iopub.status.busy":"2024-05-11T14:24:50.708267Z","iopub.execute_input":"2024-05-11T14:24:50.709046Z","iopub.status.idle":"2024-05-11T14:24:51.100294Z","shell.execute_reply.started":"2024-05-11T14:24:50.709006Z","shell.execute_reply":"2024-05-11T14:24:51.099128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features, labels, video_files_paths = np.load(\"features.npy\") , np.load(\"labels.npy\") ,  np.load(\"video_files_paths.npy\")","metadata":{"id":"9KOPtXlH3aTw","execution":{"iopub.status.busy":"2024-05-11T14:24:53.744596Z","iopub.execute_input":"2024-05-11T14:24:53.745444Z","iopub.status.idle":"2024-05-11T14:24:53.929129Z","shell.execute_reply.started":"2024-05-11T14:24:53.745411Z","shell.execute_reply":"2024-05-11T14:24:53.928133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **One-Hot Encoding**","metadata":{"id":"lkN8bgxH3aTw"}},{"cell_type":"code","source":"# convert labels into one-hot-encoded vectors\none_hot_encoded_labels = to_categorical(labels)","metadata":{"id":"SIN6V5GN3aTw","execution":{"iopub.status.busy":"2024-05-11T14:24:56.594612Z","iopub.execute_input":"2024-05-11T14:24:56.595052Z","iopub.status.idle":"2024-05-11T14:24:56.599839Z","shell.execute_reply.started":"2024-05-11T14:24:56.595018Z","shell.execute_reply":"2024-05-11T14:24:56.598766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Splitting Data To Training &Testing Sets**","metadata":{}},{"cell_type":"code","source":"# Split the Data into Train ( 90% ) and Test Set ( 10% ).\nfeatures_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.1,\n                                                                            shuffle = True, random_state = 42)","metadata":{"id":"P0uFnKvq3aTx","execution":{"iopub.status.busy":"2024-05-11T14:25:01.217367Z","iopub.execute_input":"2024-05-11T14:25:01.217746Z","iopub.status.idle":"2024-05-11T14:25:01.381808Z","shell.execute_reply.started":"2024-05-11T14:25:01.217715Z","shell.execute_reply":"2024-05-11T14:25:01.380765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(features_train.shape,labels_train.shape )\nprint(features_test.shape, labels_test.shape)","metadata":{"id":"8X0LeeVW3aTx","outputId":"52c6e405-d4a1-4688-ef79-852689bd59f1","execution":{"iopub.status.busy":"2024-05-11T14:25:01.845128Z","iopub.execute_input":"2024-05-11T14:25:01.846072Z","iopub.status.idle":"2024-05-11T14:25:01.852122Z","shell.execute_reply.started":"2024-05-11T14:25:01.846038Z","shell.execute_reply":"2024-05-11T14:25:01.851244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Importing MobileNet**","metadata":{"id":"bLGa3mfdtu_K"}},{"cell_type":"code","source":"from keras.applications.mobilenet_v2 import MobileNetV2\n\nmobilenet = MobileNetV2( include_top=False , weights=\"/kaggle/input/mobilenet/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\", )\n\n","metadata":{"id":"Tpo2Q-Uf3aT8","outputId":"fccd8ac7-0787-4b33-cf27-7a303ada1c0e","execution":{"iopub.status.busy":"2024-05-11T14:34:50.922451Z","iopub.execute_input":"2024-05-11T14:34:50.922931Z","iopub.status.idle":"2024-05-11T14:34:52.003548Z","shell.execute_reply.started":"2024-05-11T14:34:50.922895Z","shell.execute_reply":"2024-05-11T14:34:52.002643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Fine-Tuning MobileNet**","metadata":{}},{"cell_type":"code","source":"#Fine-Tuning to make the last 40 layer trainable\nmobilenet.trainable=True\n\nfor layer in mobilenet.layers[:-40]:\n  layer.trainable=False\n\nmobilenet.summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-11T14:34:57.273094Z","iopub.execute_input":"2024-05-11T14:34:57.273813Z","iopub.status.idle":"2024-05-11T14:34:57.487792Z","shell.execute_reply.started":"2024-05-11T14:34:57.273779Z","shell.execute_reply":"2024-05-11T14:34:57.486885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Building The Model**","metadata":{"id":"PFfDOTnZ3aTy"}},{"cell_type":"code","source":"def create_model():\n \n    model = Sequential()\n\n    #Specifying Input to match features shape\n    model.add(Input(shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n    \n    # Passing mobilenet in the TimeDistributed layer to handle the sequence\n    model.add(TimeDistributed(mobilenet))\n    \n    model.add(Dropout(0.25))\n                                    \n    model.add(TimeDistributed(Flatten()))\n\n    \n    lstm_fw = LSTM(units=32)\n    lstm_bw = LSTM(units=32, go_backwards = True)  \n\n    model.add(Bidirectional(lstm_fw, backward_layer = lstm_bw))\n    \n    model.add(Dropout(0.25))\n\n    model.add(Dense(256,activation='relu'))\n    model.add(Dropout(0.25))\n\n    model.add(Dense(128,activation='relu'))\n    model.add(Dropout(0.25))\n\n    model.add(Dense(64,activation='relu'))\n    model.add(Dropout(0.25))\n\n    model.add(Dense(32,activation='relu'))\n    model.add(Dropout(0.25))\n    \n    \n    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))\n \n   \n    model.summary()\n    \n    return model","metadata":{"id":"CWtYR7bM3aT9","execution":{"iopub.status.busy":"2024-05-11T14:35:06.434833Z","iopub.execute_input":"2024-05-11T14:35:06.435538Z","iopub.status.idle":"2024-05-11T14:35:06.444668Z","shell.execute_reply.started":"2024-05-11T14:35:06.435505Z","shell.execute_reply":"2024-05-11T14:35:06.443546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constructing the Model\nMoBiLSTM_model = create_model()\n","metadata":{"id":"auo9g9rpmBOS","outputId":"07c69b21-1e15-4481-cff0-fae9f9b1abef","execution":{"iopub.status.busy":"2024-05-11T14:35:13.301654Z","iopub.execute_input":"2024-05-11T14:35:13.302585Z","iopub.status.idle":"2024-05-11T14:35:13.389309Z","shell.execute_reply.started":"2024-05-11T14:35:13.302545Z","shell.execute_reply":"2024-05-11T14:35:13.388453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Fitting Model**","metadata":{"id":"c_jvIgP9wEh6"}},{"cell_type":"code","source":"import tensorflow  as tf\n# Create Early Stopping Callback to monitor the accuracy\nearly_stopping_callback = EarlyStopping(monitor = 'val_accuracy', patience = 15, restore_best_weights = True)\n\n# Create ReduceLROnPlateau Callback to reduce overfitting by decreasing learning\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                                  factor=0.6,\n                                                  patience=5,\n                                                  min_lr=0.0001,\n                                                  verbose=1)\n \n# Compiling the model \nMoBiLSTM_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = [\"accuracy\"])\n \n# Fitting the model \nMobBiLSTM_model_history = MoBiLSTM_model.fit(x = features_train, y = labels_train, epochs = 30, batch_size = 8 ,\n                                             shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback,reduce_lr])","metadata":{"id":"oA-QWlSV3aT_","outputId":"f1b4723e-1d04-4842-b5c6-29338569f694","execution":{"iopub.status.busy":"2024-05-11T14:43:17.018600Z","iopub.execute_input":"2024-05-11T14:43:17.019495Z","iopub.status.idle":"2024-05-11T14:48:00.792772Z","shell.execute_reply.started":"2024-05-11T14:43:17.019459Z","shell.execute_reply":"2024-05-11T14:48:00.791865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_evaluation_history = MoBiLSTM_model.evaluate(features_test, labels_test)","metadata":{"id":"6577H1to3aT_","outputId":"dd4650e7-7089-4931-d526-c98440b7210b","execution":{"iopub.status.busy":"2024-05-11T14:48:42.197225Z","iopub.execute_input":"2024-05-11T14:48:42.197578Z","iopub.status.idle":"2024-05-11T14:48:43.567975Z","shell.execute_reply.started":"2024-05-11T14:48:42.197550Z","shell.execute_reply":"2024-05-11T14:48:43.567032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Evaluation Of The Model**","metadata":{"id":"lPGXI4C53aUA"}},{"cell_type":"code","source":"def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):\n    \n    metric_value_1 = model_training_history.history[metric_name_1]\n    metric_value_2 = model_training_history.history[metric_name_2]\n    \n    # Get the Epochs Count\n    epochs = range(len(metric_value_1))\n \n    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)\n    plt.plot(epochs, metric_value_2, 'orange', label = metric_name_2)\n \n    plt.title(str(plot_name))\n \n    plt.legend()","metadata":{"id":"hyHkkNo93aUB","execution":{"iopub.status.busy":"2024-05-11T14:48:45.186047Z","iopub.execute_input":"2024-05-11T14:48:45.186441Z","iopub.status.idle":"2024-05-11T14:48:45.195075Z","shell.execute_reply.started":"2024-05-11T14:48:45.186408Z","shell.execute_reply":"2024-05-11T14:48:45.194130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metric(MobBiLSTM_model_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')","metadata":{"id":"-YIHQIr13aUB","outputId":"a038e7f7-56be-4c92-9ea3-0dd25de1a247","execution":{"iopub.status.busy":"2024-05-11T14:48:45.890475Z","iopub.execute_input":"2024-05-11T14:48:45.890880Z","iopub.status.idle":"2024-05-11T14:48:46.242533Z","shell.execute_reply.started":"2024-05-11T14:48:45.890846Z","shell.execute_reply":"2024-05-11T14:48:46.241518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metric(MobBiLSTM_model_history, 'accuracy', 'val_accuracy', 'Total Loss vs Total Validation Loss')","metadata":{"id":"JUeYKvq23aUB","outputId":"b19327aa-bc13-45ad-d7db-5fccedd0f2aa","execution":{"iopub.status.busy":"2024-05-11T14:48:50.510282Z","iopub.execute_input":"2024-05-11T14:48:50.510649Z","iopub.status.idle":"2024-05-11T14:48:50.845572Z","shell.execute_reply.started":"2024-05-11T14:48:50.510619Z","shell.execute_reply":"2024-05-11T14:48:50.844726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predicting the Test Set**","metadata":{"id":"f8rpSPRGxzLM"}},{"cell_type":"code","source":"labels_predict = MoBiLSTM_model.predict(features_test)","metadata":{"id":"BCuJOPWn3aUC","execution":{"iopub.status.busy":"2024-05-11T14:48:54.443422Z","iopub.execute_input":"2024-05-11T14:48:54.443875Z","iopub.status.idle":"2024-05-11T14:49:31.260128Z","shell.execute_reply.started":"2024-05-11T14:48:54.443829Z","shell.execute_reply":"2024-05-11T14:49:31.259279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decoding the data to use in Metrics\nlabels_predict = np.argmax(labels_predict , axis=1)\nlabels_test_normal = np.argmax(labels_test , axis=1)","metadata":{"id":"6PtV_By23aUC","execution":{"iopub.status.busy":"2024-05-11T14:49:31.262584Z","iopub.execute_input":"2024-05-11T14:49:31.262975Z","iopub.status.idle":"2024-05-11T14:49:31.267950Z","shell.execute_reply.started":"2024-05-11T14:49:31.262947Z","shell.execute_reply":"2024-05-11T14:49:31.266968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_test_normal.shape , labels_predict.shape","metadata":{"id":"HP-sdFAj3aUC","outputId":"96fbaf31-208f-4271-e83d-36b2b5cb07cc","execution":{"iopub.status.busy":"2024-05-11T14:49:31.269041Z","iopub.execute_input":"2024-05-11T14:49:31.269311Z","iopub.status.idle":"2024-05-11T14:49:31.279911Z","shell.execute_reply.started":"2024-05-11T14:49:31.269288Z","shell.execute_reply":"2024-05-11T14:49:31.279079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accuracy Score","metadata":{"id":"JR45ydemygm6"}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nAccScore = accuracy_score(labels_predict, labels_test_normal)\nprint('Accuracy Score is : ', AccScore)","metadata":{"id":"99mVAuNu3aUC","outputId":"fc39c654-e5bb-4977-c840-fa4695dde3c5","execution":{"iopub.status.busy":"2024-05-11T14:49:37.503428Z","iopub.execute_input":"2024-05-11T14:49:37.503836Z","iopub.status.idle":"2024-05-11T14:49:37.510943Z","shell.execute_reply.started":"2024-05-11T14:49:37.503805Z","shell.execute_reply":"2024-05-11T14:49:37.509794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Confusion Matrix","metadata":{"id":"FMtS_I_MyiY5"}},{"cell_type":"code","source":"import seaborn as sns \nfrom sklearn.metrics import confusion_matrix\n\nax= plt.subplot()\ncm=confusion_matrix(labels_test_normal, labels_predict)\nsns.heatmap(cm, annot=True, fmt='g', ax=ax);  \n\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['True', 'False']); ax.yaxis.set_ticklabels(['NonViolence', 'Violence']);","metadata":{"id":"xzg5AVTC3aUD","outputId":"d469d340-b976-4858-9740-b0ae9e3b9883","execution":{"iopub.status.busy":"2024-05-11T14:49:41.129310Z","iopub.execute_input":"2024-05-11T14:49:41.130201Z","iopub.status.idle":"2024-05-11T14:49:41.402201Z","shell.execute_reply.started":"2024-05-11T14:49:41.130166Z","shell.execute_reply":"2024-05-11T14:49:41.401256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Classification Report","metadata":{"id":"zX8EOmSTypNq"}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nClassificationReport = classification_report(labels_test_normal,labels_predict)\nprint('Classification Report is : \\n', ClassificationReport)","metadata":{"id":"pcWhopTm3aUD","outputId":"aef35268-bd38-4755-9096-9e5dcfb6244b","execution":{"iopub.status.busy":"2024-05-11T14:49:45.804871Z","iopub.execute_input":"2024-05-11T14:49:45.805752Z","iopub.status.idle":"2024-05-11T14:49:45.821260Z","shell.execute_reply.started":"2024-05-11T14:49:45.805718Z","shell.execute_reply":"2024-05-11T14:49:45.820435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Prediction Frame By Frame**","metadata":{"id":"p9kjBjoN3aUD"}},{"cell_type":"code","source":"def predict_frames(video_file_path, output_file_path, SEQUENCE_LENGTH):\n    \n    # Read from the video file.\n    video_reader = cv2.VideoCapture(video_file_path)\n \n    # Get the width and height of the video.\n    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n \n    # VideoWriter to store the output video in the disk.\n    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('m', 'p', '4', 'v'), \n                                    video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))\n \n    # Declare a queue to store video frames.\n    frames_queue = deque(maxlen = SEQUENCE_LENGTH)\n \n    # Store the predicted class in the video.\n    predicted_class_name = ''\n \n    # Iterate until the video is accessed successfully.\n    while video_reader.isOpened():\n \n        ok, frame = video_reader.read() \n        \n        if not ok:\n            break\n \n        # Resize the Frame to fixed Dimensions.\n        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        \n        # Normalize the resized frame \n        normalized_frame = resized_frame / 255\n \n        # Appending the pre-processed frame into the frames list.\n        frames_queue.append(normalized_frame)\n \n        # We Need at Least number of SEQUENCE_LENGTH Frames to perform a prediction.\n        # Check if the number of frames in the queue are equal to the fixed sequence length.\n        if len(frames_queue) == SEQUENCE_LENGTH:                        \n \n            # Pass the normalized frames to the model and get the predicted probabilities.\n            predicted_labels_probabilities = MoBiLSTM_model.predict(np.expand_dims(frames_queue, axis = 0))[0]\n \n            # Get the index of class with highest probability.\n            predicted_label = np.argmax(predicted_labels_probabilities)\n \n            # Get the class name using the retrieved index.\n            predicted_class_name = CLASSES_LIST[predicted_label]\n \n        # Write predicted class name on top of the frame.\n        if predicted_class_name == \"violent\":\n            cv2.putText(frame, predicted_class_name, (5, 100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 255), 12)\n        else:\n            cv2.putText(frame, predicted_class_name, (5, 100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 0), 12)\n         \n        # Write The frame into the disk using the VideoWriter\n        video_writer.write(frame)                       \n        \n    video_reader.release()\n    video_writer.release()","metadata":{"id":"90aMcgLB3aUD","execution":{"iopub.status.busy":"2024-05-11T14:49:51.579670Z","iopub.execute_input":"2024-05-11T14:49:51.580413Z","iopub.status.idle":"2024-05-11T14:49:51.592684Z","shell.execute_reply.started":"2024-05-11T14:49:51.580377Z","shell.execute_reply":"2024-05-11T14:49:51.591602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use(\"default\")\n\n# To show Random Frames from the saved output predicted video (output predicted video doesn't show on the notebook but can be downloaded)\ndef show_pred_frames(pred_video_path): \n\n    plt.figure(figsize=(20,15))\n\n    video_reader = cv2.VideoCapture(pred_video_path)\n\n    # Get the number of frames in the video.\n    frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    # Get Random Frames from the video then Sort it\n    random_range = sorted(random.sample(range (SEQUENCE_LENGTH , frames_count ), 12))\n        \n    for counter, random_index in enumerate(random_range, 1):\n        \n        plt.subplot(5, 4, counter)\n\n        # Set the current frame position of the video.  \n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, random_index)\n          \n        ok, frame = video_reader.read() \n\n        if not ok:\n          break \n\n        frame = cv2.cvtColor(frame , cv2.COLOR_BGR2RGB)\n\n        plt.imshow(frame);ax.figure.set_size_inches(20,20);plt.tight_layout()\n                            \n    video_reader.release()","metadata":{"id":"RMzc9v5T3aUD","execution":{"iopub.status.busy":"2024-05-11T14:49:52.436535Z","iopub.execute_input":"2024-05-11T14:49:52.437438Z","iopub.status.idle":"2024-05-11T14:49:52.446997Z","shell.execute_reply.started":"2024-05-11T14:49:52.437406Z","shell.execute_reply":"2024-05-11T14:49:52.445999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Construct the output video path.\ntest_videos_directory = 'test_videos'\nos.makedirs(test_videos_directory, exist_ok = True)\n \noutput_video_file_path = f'{test_videos_directory}/Output-Test-Video.mp4'","metadata":{"id":"TLoHFIZG1nfx","execution":{"iopub.status.busy":"2024-05-11T14:50:52.856374Z","iopub.execute_input":"2024-05-11T14:50:52.857296Z","iopub.status.idle":"2024-05-11T14:50:52.861837Z","shell.execute_reply.started":"2024-05-11T14:50:52.857260Z","shell.execute_reply":"2024-05-11T14:50:52.860904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specifying video to be predicted\ninput_video_file_path = r\"/kaggle/input/violencedetectiondataset/violence-detection-dataset/violent/12.mp4\"\n\n# Perform Prediction on the Test Video.\npredict_frames(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)\n\n# Show random frames from the output video\nshow_pred_frames(output_video_file_path)","metadata":{"id":"zXBFNGU83aUE","outputId":"53ee1964-9144-4a75-994b-df86e3b71018","execution":{"iopub.status.busy":"2024-05-11T14:50:55.275227Z","iopub.execute_input":"2024-05-11T14:50:55.275927Z","iopub.status.idle":"2024-05-11T14:51:28.550233Z","shell.execute_reply.started":"2024-05-11T14:50:55.275886Z","shell.execute_reply":"2024-05-11T14:51:28.549082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Play the actual video\nPlay_Video(input_video_file_path)","metadata":{"id":"lYbKAUan3aUE","outputId":"6dd94528-4e86-4ec7-e5db-f8b27cde68ef","execution":{"iopub.status.busy":"2024-05-11T14:52:03.120720Z","iopub.execute_input":"2024-05-11T14:52:03.121086Z","iopub.status.idle":"2024-05-11T14:52:03.182913Z","shell.execute_reply.started":"2024-05-11T14:52:03.121057Z","shell.execute_reply":"2024-05-11T14:52:03.181415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Prediction For Video**","metadata":{"id":"Q4-IYON63aUE"}},{"cell_type":"code","source":"def predict_video(video_file_path, SEQUENCE_LENGTH):\n \n    video_reader = cv2.VideoCapture(video_file_path)\n \n    # Get the width and height of the video.\n    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n \n    # Declare a list to store video frames we will extract.\n    frames_list = []\n    \n    # Store the predicted class in the video.\n    predicted_class_name = ''\n \n    # Get the number of frames in the video.\n    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n \n    # Calculate the interval after which frames will be added to the list.\n    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH),1)\n \n    # Iterating the number of times equal to the fixed length of sequence.\n    for frame_counter in range(SEQUENCE_LENGTH):\n \n        # Set the current frame position of the video.\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n \n        success, frame = video_reader.read() \n \n        if not success:\n            break\n \n        # Resize the Frame to fixed Dimensions.\n        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        \n        # Normalize the resized frame.\n        normalized_frame = resized_frame / 255\n        \n        # Appending the pre-processed frame into the frames list\n        frames_list.append(normalized_frame)\n \n    # Passing the  pre-processed frames to the model and get the predicted probabilities.\n    predicted_labels_probabilities = MoBiLSTM_model.predict(np.expand_dims(frames_list, axis = 0))[0]\n \n    # Get the index of class with highest probability.\n    predicted_label = np.argmax(predicted_labels_probabilities)\n \n    # Get the class name using the retrieved index.\n    predicted_class_name = CLASSES_LIST[predicted_label]\n    \n    # Display the predicted class along with the prediction confidence.\n    print(f'Predicted: {predicted_class_name}\\nConfidence: {predicted_labels_probabilities[predicted_label]}')\n        \n    video_reader.release()","metadata":{"id":"p4gjtz-73aUE","execution":{"iopub.status.busy":"2024-05-11T14:52:09.853600Z","iopub.execute_input":"2024-05-11T14:52:09.854004Z","iopub.status.idle":"2024-05-11T14:52:09.864360Z","shell.execute_reply.started":"2024-05-11T14:52:09.853971Z","shell.execute_reply":"2024-05-11T14:52:09.863220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specifying video to be predicted\ninput_video_file_path = r\"/kaggle/input/violencedetectiondataset/violence-detection-dataset/violent/115.mp4\"\n\n# Perform Single Prediction on the Test Video.\npredict_video(input_video_file_path, SEQUENCE_LENGTH)\n\n# Play the actual video\nPlay_Video(input_video_file_path)","metadata":{"id":"O6DQ5V-b3aUF","outputId":"2509c4e5-de79-4431-d18f-7caf178b4d01","execution":{"iopub.status.busy":"2024-05-11T14:52:11.011786Z","iopub.execute_input":"2024-05-11T14:52:11.012159Z","iopub.status.idle":"2024-05-11T14:52:16.349729Z","shell.execute_reply.started":"2024-05-11T14:52:11.012128Z","shell.execute_reply":"2024-05-11T14:52:16.348672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specifying video to be predicted\ninput_video_file_path = r\"/kaggle/input/violencedetectiondataset/violence-detection-dataset/non-violent/23.mp4\"\n\n# Perform Single Prediction on the Test Video.\npredict_video(input_video_file_path, SEQUENCE_LENGTH)\n\n# Play the actual video\nPlay_Video(input_video_file_path)","metadata":{"id":"nxwWGvUg3aUF","outputId":"576775a3-d10a-4782-85d7-d809d6a845f7","execution":{"iopub.status.busy":"2024-05-11T14:52:25.663150Z","iopub.execute_input":"2024-05-11T14:52:25.663521Z","iopub.status.idle":"2024-05-11T14:52:32.452974Z","shell.execute_reply.started":"2024-05-11T14:52:25.663489Z","shell.execute_reply":"2024-05-11T14:52:32.450810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}